{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78be5c07-dcd1-4690-b40a-65cbffe414b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--output_dir OUTPUT_DIR]\n",
      "                             [--frame_skip FRAME_SKIP]\n",
      "                             [--start_frame START_FRAME]\n",
      "                             [--end_frame END_FRAME] [--resize WIDTH HEIGHT]\n",
      "                             [--output_format {numpy,tensor,both}]\n",
      "                             [--normalize] [--device DEVICE] [--demo]\n",
      "                             [--demo_tensor]\n",
      "                             video_path\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Video Frame Iterator with Metadata Collection\n",
    "\n",
    "This module provides an iterator that processes video files frame by frame,\n",
    "collecting metadata associated with each frame.\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    import cv2\n",
    "    CV2_AVAILABLE = True\n",
    "except ImportError:\n",
    "    CV2_AVAILABLE = False\n",
    "    print(\"Warning: OpenCV not available. Install with: pip install opencv-python\")\n",
    "\n",
    "try:\n",
    "    import numpy as np\n",
    "    NUMPY_AVAILABLE = True\n",
    "except ImportError:\n",
    "    NUMPY_AVAILABLE = False\n",
    "    print(\"Warning: NumPy not available. Install with: pip install numpy\")\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn.functional as F\n",
    "    TORCH_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TORCH_AVAILABLE = False\n",
    "    print(\"Warning: PyTorch not available. Install with: pip install torch\")\n",
    "\n",
    "from typing import Iterator, Dict, Any, Optional, Union, Tuple\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "class VideoFrameMetadata:\n",
    "    \"\"\"Container for frame metadata\"\"\"\n",
    "    \n",
    "    def __init__(self, frame_number: int, timestamp: float, video_info: Dict[str, Any]):\n",
    "        self.frame_number = frame_number\n",
    "        self.timestamp = timestamp  # Timestamp in seconds\n",
    "        self.video_info = video_info\n",
    "        self.processing_time = None\n",
    "        self.custom_metadata = {}\n",
    "    \n",
    "    def add_custom_metadata(self, key: str, value: Any):\n",
    "        \"\"\"Add custom metadata to this frame\"\"\"\n",
    "        self.custom_metadata[key] = value\n",
    "    \n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        \"\"\"Convert metadata to dictionary\"\"\"\n",
    "        return {\n",
    "            'frame_number': self.frame_number,\n",
    "            'timestamp': self.timestamp,\n",
    "            'timestamp_formatted': str(timedelta(seconds=self.timestamp)),\n",
    "            'video_info': self.video_info,\n",
    "            'processing_time': self.processing_time,\n",
    "            'custom_metadata': self.custom_metadata\n",
    "        }\n",
    "\n",
    "\n",
    "class VideoFrameIterator:\n",
    "    \"\"\"\n",
    "    Iterator that processes video files frame by frame with metadata collection\n",
    "    \n",
    "    Features:\n",
    "    - Frame-by-frame iteration\n",
    "    - Metadata collection for each frame\n",
    "    - Support for various video formats\n",
    "    - Optional frame skipping and sampling\n",
    "    - Memory-efficient processing\n",
    "    - Progress tracking\n",
    "    - Tensor output support for deep learning\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        video_path: Union[str, Path],\n",
    "        frame_skip: int = 1,\n",
    "        start_frame: int = 0,\n",
    "        end_frame: Optional[int] = None,\n",
    "        collect_frame_stats: bool = True,\n",
    "        resize_frames: Optional[Tuple[int, int]] = None,\n",
    "        output_format: str = \"numpy\",  # \"numpy\", \"tensor\", \"both\"\n",
    "        normalize: bool = True,\n",
    "        device: str = \"cpu\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the video frame iterator\n",
    "        \n",
    "        Args:\n",
    "            video_path: Path to the video file\n",
    "            frame_skip: Skip every N frames (1 = process all frames)\n",
    "            start_frame: Starting frame number\n",
    "            end_frame: Ending frame number (None = process until end)\n",
    "            collect_frame_stats: Whether to collect statistical metadata for each frame\n",
    "            resize_frames: Optional tuple (width, height) to resize frames\n",
    "            output_format: Output format - \"numpy\", \"tensor\", or \"both\"\n",
    "            normalize: Whether to normalize pixel values to [0, 1] range\n",
    "            device: Device for tensor operations (\"cpu\", \"cuda\", etc.)\n",
    "        \"\"\"\n",
    "        self.video_path = Path(video_path)\n",
    "        self.frame_skip = max(1, frame_skip)\n",
    "        self.start_frame = max(0, start_frame)\n",
    "        self.end_frame = end_frame\n",
    "        self.collect_frame_stats = collect_frame_stats\n",
    "        self.resize_frames = resize_frames\n",
    "        self.output_format = output_format.lower()\n",
    "        self.normalize = normalize\n",
    "        self.device = device\n",
    "        \n",
    "        # Validate output format\n",
    "        if self.output_format not in [\"numpy\", \"tensor\", \"both\"]:\n",
    "            raise ValueError(\"output_format must be 'numpy', 'tensor', or 'both'\")\n",
    "        \n",
    "        # Check dependencies\n",
    "        if not CV2_AVAILABLE:\n",
    "            raise ImportError(\"OpenCV is required for video processing. Install with: pip install opencv-python\")\n",
    "        if not NUMPY_AVAILABLE:\n",
    "            raise ImportError(\"NumPy is required for video processing. Install with: pip install numpy\")\n",
    "        \n",
    "        if self.output_format in [\"tensor\", \"both\"] and not TORCH_AVAILABLE:\n",
    "            raise ImportError(\"PyTorch is required for tensor output. Install with: pip install torch\")\n",
    "        \n",
    "        # Initialize video capture\n",
    "        self.cap = cv2.VideoCapture(str(self.video_path))\n",
    "        if not self.cap.isOpened():\n",
    "            raise ValueError(f\"Could not open video file: {self.video_path}\")\n",
    "        \n",
    "        # Get video information\n",
    "        self.video_info = self._get_video_info()\n",
    "        \n",
    "        # Set starting position\n",
    "        if self.start_frame > 0:\n",
    "            self.cap.set(cv2.CAP_PROP_POS_FRAMES, self.start_frame)\n",
    "        \n",
    "        self.current_frame_number = self.start_frame\n",
    "        self.frames_processed = 0\n",
    "        \n",
    "    def _get_video_info(self) -> Dict[str, Any]:\n",
    "        \"\"\"Extract video metadata\"\"\"\n",
    "        return {\n",
    "            'filename': self.video_path.name,\n",
    "            'path': str(self.video_path),\n",
    "            'total_frames': int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT)),\n",
    "            'fps': self.cap.get(cv2.CAP_PROP_FPS),\n",
    "            'width': int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH)),\n",
    "            'height': int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT)),\n",
    "            'duration_seconds': int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT)) / self.cap.get(cv2.CAP_PROP_FPS),\n",
    "            'fourcc': int(self.cap.get(cv2.CAP_PROP_FOURCC)),\n",
    "            'codec': self._fourcc_to_string(int(self.cap.get(cv2.CAP_PROP_FOURCC)))\n",
    "        }\n",
    "    \n",
    "    def _fourcc_to_string(self, fourcc: int) -> str:\n",
    "        \"\"\"Convert FOURCC code to string\"\"\"\n",
    "        return \"\".join([chr((fourcc >> 8 * i) & 0xFF) for i in range(4)])\n",
    "    \n",
    "    def _numpy_to_tensor(self, frame: np.ndarray) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Convert numpy frame to PyTorch tensor\n",
    "        \n",
    "        Args:\n",
    "            frame: Input frame as numpy array (H, W, C) in BGR format\n",
    "            \n",
    "        Returns:\n",
    "            Tensor in format (C, H, W) with RGB channel order\n",
    "        \"\"\"\n",
    "        # Convert BGR to RGB\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Normalize to [0, 1] if requested\n",
    "        if self.normalize:\n",
    "            frame_rgb = frame_rgb.astype(np.float32) / 255.0\n",
    "        \n",
    "        # Convert to tensor and change from HWC to CHW\n",
    "        tensor = torch.from_numpy(frame_rgb.transpose(2, 0, 1))\n",
    "        \n",
    "        # Move to specified device\n",
    "        if self.device != \"cpu\":\n",
    "            tensor = tensor.to(self.device)\n",
    "            \n",
    "        return tensor\n",
    "    \n",
    "    def _process_frame_output(self, frame: np.ndarray) -> Union[np.ndarray, torch.Tensor, Tuple]:\n",
    "        \"\"\"\n",
    "        Process frame according to output format\n",
    "        \n",
    "        Args:\n",
    "            frame: Input frame as numpy array\n",
    "            \n",
    "        Returns:\n",
    "            Frame in requested format(s)\n",
    "        \"\"\"\n",
    "        if self.output_format == \"numpy\":\n",
    "            return frame\n",
    "        elif self.output_format == \"tensor\":\n",
    "            return self._numpy_to_tensor(frame)\n",
    "        elif self.output_format == \"both\":\n",
    "            tensor = self._numpy_to_tensor(frame)\n",
    "            return frame, tensor\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown output format: {self.output_format}\")\n",
    "    \n",
    "    def _collect_frame_statistics(self, frame) -> Dict[str, Any]:\n",
    "        \"\"\"Collect statistical metadata for a frame\"\"\"\n",
    "        if not self.collect_frame_stats:\n",
    "            return {}\n",
    "        \n",
    "        # Convert to different color spaces for analysis\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "        \n",
    "        stats = {\n",
    "            'shape': frame.shape,\n",
    "            'dtype': str(frame.dtype),\n",
    "            'mean_brightness': float(np.mean(gray)),\n",
    "            'std_brightness': float(np.std(gray)),\n",
    "            'min_brightness': int(np.min(gray)),\n",
    "            'max_brightness': int(np.max(gray)),\n",
    "            'mean_rgb': [float(np.mean(frame[:,:,i])) for i in range(3)],\n",
    "            'std_rgb': [float(np.std(frame[:,:,i])) for i in range(3)],\n",
    "            'mean_hue': float(np.mean(hsv[:,:,0])),\n",
    "            'mean_saturation': float(np.mean(hsv[:,:,1])),\n",
    "            'mean_value': float(np.mean(hsv[:,:,2])),\n",
    "            'file_size_bytes': frame.nbytes,\n",
    "            'unique_colors': len(np.unique(frame.reshape(-1, frame.shape[-1]), axis=0))\n",
    "        }\n",
    "        \n",
    "        # Add tensor-specific metadata if tensor output is requested\n",
    "        if self.output_format in [\"tensor\", \"both\"] and TORCH_AVAILABLE:\n",
    "            tensor = self._numpy_to_tensor(frame)\n",
    "            stats.update({\n",
    "                'tensor_shape': list(tensor.shape),\n",
    "                'tensor_dtype': str(tensor.dtype),\n",
    "                'tensor_device': str(tensor.device),\n",
    "                'tensor_min': float(tensor.min()),\n",
    "                'tensor_max': float(tensor.max()),\n",
    "                'tensor_mean': float(tensor.mean()),\n",
    "                'tensor_std': float(tensor.std())\n",
    "            })\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def __iter__(self):\n",
    "        \"\"\"Make this object iterable\"\"\"\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        \"\"\"\n",
    "        Get the next frame with metadata\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (frame_array, metadata_object)\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Check if we've reached the end\n",
    "        if self.end_frame is not None and self.current_frame_number >= self.end_frame:\n",
    "            self.cap.release()\n",
    "            raise StopIteration\n",
    "        \n",
    "        # Read frame\n",
    "        ret, frame = self.cap.read()\n",
    "        if not ret:\n",
    "            self.cap.release()\n",
    "            raise StopIteration\n",
    "        \n",
    "        # Calculate timestamp\n",
    "        timestamp = self.current_frame_number / self.video_info['fps']\n",
    "        \n",
    "        # Create metadata object\n",
    "        metadata = VideoFrameMetadata(\n",
    "            frame_number=self.current_frame_number,\n",
    "            timestamp=timestamp,\n",
    "            video_info=self.video_info\n",
    "        )\n",
    "        \n",
    "        # Resize frame if requested\n",
    "        if self.resize_frames:\n",
    "            frame = cv2.resize(frame, self.resize_frames)\n",
    "            metadata.add_custom_metadata('resized_to', self.resize_frames)\n",
    "            metadata.add_custom_metadata('original_size', (self.video_info['width'], self.video_info['height']))\n",
    "        \n",
    "        # Collect frame statistics (before tensor conversion)\n",
    "        frame_stats = self._collect_frame_statistics(frame)\n",
    "        for key, value in frame_stats.items():\n",
    "            metadata.add_custom_metadata(key, value)\n",
    "        \n",
    "        # Convert frame to requested output format\n",
    "        processed_frame = self._process_frame_output(frame)\n",
    "        \n",
    "        # Record processing time\n",
    "        metadata.processing_time = time.time() - start_time\n",
    "        \n",
    "        # Skip frames if needed\n",
    "        for _ in range(self.frame_skip - 1):\n",
    "            ret, _ = self.cap.read()\n",
    "            if not ret:\n",
    "                self.cap.release()\n",
    "                raise StopIteration\n",
    "            self.current_frame_number += 1\n",
    "        \n",
    "        self.current_frame_number += 1\n",
    "        self.frames_processed += 1\n",
    "        \n",
    "        return processed_frame, metadata\n",
    "    \n",
    "    def get_frame_at_time(self, timestamp_seconds: float):\n",
    "        \"\"\"\n",
    "        Get a specific frame at a given timestamp\n",
    "        \n",
    "        Args:\n",
    "            timestamp_seconds: Time in seconds to seek to\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (frame_array, metadata_object)\n",
    "        \"\"\"\n",
    "        frame_number = int(timestamp_seconds * self.video_info['fps'])\n",
    "        return self.get_frame_at_number(frame_number)\n",
    "    \n",
    "    def get_frame_at_number(self, frame_number: int):\n",
    "        \"\"\"\n",
    "        Get a specific frame by frame number\n",
    "        \n",
    "        Args:\n",
    "            frame_number: Frame number to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (frame_array, metadata_object)\n",
    "        \"\"\"\n",
    "        if frame_number < 0 or frame_number >= self.video_info['total_frames']:\n",
    "            raise ValueError(f\"Frame number {frame_number} is out of range [0, {self.video_info['total_frames']})\")\n",
    "        \n",
    "        # Seek to the frame\n",
    "        self.cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "        \n",
    "        ret, frame = self.cap.read()\n",
    "        if not ret:\n",
    "            raise ValueError(f\"Could not read frame {frame_number}\")\n",
    "        \n",
    "        timestamp = frame_number / self.video_info['fps']\n",
    "        \n",
    "        metadata = VideoFrameMetadata(\n",
    "            frame_number=frame_number,\n",
    "            timestamp=timestamp,\n",
    "            video_info=self.video_info\n",
    "        )\n",
    "        \n",
    "        # Resize frame if requested\n",
    "        if self.resize_frames:\n",
    "            frame = cv2.resize(frame, self.resize_frames)\n",
    "            metadata.add_custom_metadata('resized_to', self.resize_frames)\n",
    "        \n",
    "        # Collect frame statistics\n",
    "        frame_stats = self._collect_frame_statistics(frame)\n",
    "        for key, value in frame_stats.items():\n",
    "            metadata.add_custom_metadata(key, value)\n",
    "        \n",
    "        # Convert frame to requested output format\n",
    "        processed_frame = self._process_frame_output(frame)\n",
    "        \n",
    "        return processed_frame, metadata\n",
    "    \n",
    "    def save_metadata_batch(self, metadata_list: list, output_path: Union[str, Path]):\n",
    "        \"\"\"\n",
    "        Save a batch of metadata to JSON file\n",
    "        \n",
    "        Args:\n",
    "            metadata_list: List of VideoFrameMetadata objects\n",
    "            output_path: Path to save the JSON file\n",
    "        \"\"\"\n",
    "        output_path = Path(output_path)\n",
    "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        metadata_dicts = [metadata.to_dict() for metadata in metadata_list]\n",
    "        \n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(metadata_dicts, f, indent=2, default=str)\n",
    "    \n",
    "    def get_progress_info(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get current progress information\"\"\"\n",
    "        total_frames = self.video_info['total_frames']\n",
    "        if self.end_frame:\n",
    "            total_frames = min(total_frames, self.end_frame) - self.start_frame\n",
    "        \n",
    "        progress_percentage = (self.frames_processed * self.frame_skip) / total_frames * 100\n",
    "        \n",
    "        return {\n",
    "            'current_frame': self.current_frame_number,\n",
    "            'frames_processed': self.frames_processed,\n",
    "            'total_frames': total_frames,\n",
    "            'progress_percentage': progress_percentage,\n",
    "            'estimated_remaining_frames': total_frames - (self.frames_processed * self.frame_skip)\n",
    "        }\n",
    "    \n",
    "    def __enter__(self):\n",
    "        \"\"\"Context manager entry\"\"\"\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        \"\"\"Context manager exit - cleanup resources\"\"\"\n",
    "        if self.cap:\n",
    "            self.cap.release()\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Manually close the video capture\"\"\"\n",
    "        if self.cap:\n",
    "            self.cap.release()\n",
    "\n",
    "\n",
    "class TensorFrameBatcher:\n",
    "    \"\"\"\n",
    "    Utility class to batch frames into tensors for deep learning\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, batch_size: int = 8, device: str = \"cpu\"):\n",
    "        \"\"\"\n",
    "        Initialize tensor batcher\n",
    "        \n",
    "        Args:\n",
    "            batch_size: Number of frames to batch together\n",
    "            device: Device to place tensors on\n",
    "        \"\"\"\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.current_batch = []\n",
    "        self.current_metadata = []\n",
    "    \n",
    "    def add_frame(self, frame_tensor: torch.Tensor, metadata: VideoFrameMetadata):\n",
    "        \"\"\"Add a frame tensor to the current batch\"\"\"\n",
    "        if not TORCH_AVAILABLE:\n",
    "            raise ImportError(\"PyTorch is required for tensor batching\")\n",
    "        \n",
    "        self.current_batch.append(frame_tensor)\n",
    "        self.current_metadata.append(metadata)\n",
    "        \n",
    "        # Return batch if it's full\n",
    "        if len(self.current_batch) >= self.batch_size:\n",
    "            return self.get_batch()\n",
    "        \n",
    "        return None, None\n",
    "    \n",
    "    def get_batch(self):\n",
    "        \"\"\"Get the current batch as a stacked tensor\"\"\"\n",
    "        if not self.current_batch:\n",
    "            return None, None\n",
    "        \n",
    "        # Stack tensors\n",
    "        batch_tensor = torch.stack(self.current_batch).to(self.device)\n",
    "        batch_metadata = self.current_metadata.copy()\n",
    "        \n",
    "        # Clear current batch\n",
    "        self.current_batch.clear()\n",
    "        self.current_metadata.clear()\n",
    "        \n",
    "        return batch_tensor, batch_metadata\n",
    "    \n",
    "    def get_remaining(self):\n",
    "        \"\"\"Get any remaining frames in the batch\"\"\"\n",
    "        if self.current_batch:\n",
    "            return self.get_batch()\n",
    "        return None, None\n",
    "\n",
    "\n",
    "class BatchVideoProcessor:\n",
    "    \"\"\"\n",
    "    Process multiple videos and collect metadata in batches\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, video_paths: list, **iterator_kwargs):\n",
    "        \"\"\"\n",
    "        Initialize batch processor\n",
    "        \n",
    "        Args:\n",
    "            video_paths: List of video file paths\n",
    "            **iterator_kwargs: Arguments passed to VideoFrameIterator\n",
    "        \"\"\"\n",
    "        self.video_paths = [Path(p) for p in video_paths]\n",
    "        self.iterator_kwargs = iterator_kwargs\n",
    "        self.all_metadata = []\n",
    "    \n",
    "    def process_all(self, metadata_callback: Optional[callable] = None) -> Dict[str, list]:\n",
    "        \"\"\"\n",
    "        Process all videos and collect metadata\n",
    "        \n",
    "        Args:\n",
    "            metadata_callback: Optional callback function called for each frame\n",
    "                             Signature: callback(frame, metadata) -> modified_metadata\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with video paths as keys and metadata lists as values\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for video_path in self.video_paths:\n",
    "            print(f\"Processing video: {video_path}\")\n",
    "            video_metadata = []\n",
    "            \n",
    "            try:\n",
    "                with VideoFrameIterator(video_path, **self.iterator_kwargs) as iterator:\n",
    "                    for frame, metadata in iterator:\n",
    "                        # Apply callback if provided\n",
    "                        if metadata_callback:\n",
    "                            metadata = metadata_callback(frame, metadata)\n",
    "                        \n",
    "                        video_metadata.append(metadata)\n",
    "                        \n",
    "                        # Print progress every 100 frames\n",
    "                        if len(video_metadata) % 100 == 0:\n",
    "                            progress = iterator.get_progress_info()\n",
    "                            print(f\"  Processed {progress['frames_processed']} frames \"\n",
    "                                  f\"({progress['progress_percentage']:.1f}%)\")\n",
    "                \n",
    "                results[str(video_path)] = video_metadata\n",
    "                print(f\"  Completed: {len(video_metadata)} frames processed\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  Error processing {video_path}: {e}\")\n",
    "                results[str(video_path)] = []\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def save_all_metadata(self, output_dir: Union[str, Path]):\n",
    "        \"\"\"Save metadata for all processed videos\"\"\"\n",
    "        output_dir = Path(output_dir)\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        results = self.process_all()\n",
    "        \n",
    "        for video_path, metadata_list in results.items():\n",
    "            video_name = Path(video_path).stem\n",
    "            output_file = output_dir / f\"{video_name}_metadata.json\"\n",
    "            \n",
    "            metadata_dicts = [metadata.to_dict() for metadata in metadata_list]\n",
    "            with open(output_file, 'w') as f:\n",
    "                json.dump(metadata_dicts, f, indent=2, default=str)\n",
    "            \n",
    "            print(f\"Saved metadata for {video_name}: {output_file}\")\n",
    "\n",
    "\n",
    "# Example usage and testing functions\n",
    "def example_metadata_callback(frame, metadata: VideoFrameMetadata) -> VideoFrameMetadata:\n",
    "    \"\"\"\n",
    "    Example callback function that adds custom analysis to frame metadata\n",
    "    \"\"\"\n",
    "    # Add motion detection\n",
    "    if hasattr(example_metadata_callback, 'prev_frame'):\n",
    "        diff = cv2.absdiff(frame, example_metadata_callback.prev_frame)\n",
    "        motion_score = np.mean(diff)\n",
    "        metadata.add_custom_metadata('motion_score', float(motion_score))\n",
    "    \n",
    "    example_metadata_callback.prev_frame = frame.copy()\n",
    "    \n",
    "    # Add edge detection score\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    edges = cv2.Canny(gray, 50, 150)\n",
    "    edge_density = np.sum(edges > 0) / edges.size\n",
    "    metadata.add_custom_metadata('edge_density', float(edge_density))\n",
    "    \n",
    "    # Add color dominance\n",
    "    colors = frame.reshape(-1, 3)\n",
    "    dominant_color = np.mean(colors, axis=0)\n",
    "    metadata.add_custom_metadata('dominant_color_bgr', dominant_color.tolist())\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "\n",
    "def demo_tensor_processing(video_path: str, output_dir: str = \"video_metadata_output\"):\n",
    "    \"\"\"\n",
    "    Demonstration of video frame processing with tensor output\n",
    "    \n",
    "    Args:\n",
    "        video_path: Path to video file\n",
    "        output_dir: Directory to save metadata\n",
    "    \"\"\"\n",
    "    print(f\"Demo: Processing video {video_path} with tensor output\")\n",
    "    \n",
    "    # Example 1: Tensor output\n",
    "    print(\"\\n=== Tensor Frame Processing ===\")\n",
    "    try:\n",
    "        with VideoFrameIterator(\n",
    "            video_path, \n",
    "            frame_skip=10,\n",
    "            output_format=\"tensor\",\n",
    "            normalize=True,\n",
    "            resize_frames=(224, 224),  # Common size for deep learning\n",
    "            device=\"cpu\"\n",
    "        ) as iterator:\n",
    "            \n",
    "            for i, (frame_tensor, metadata) in enumerate(iterator):\n",
    "                print(f\"Frame {metadata.frame_number}: \"\n",
    "                      f\"Tensor shape {frame_tensor.shape}, \"\n",
    "                      f\"dtype {frame_tensor.dtype}, \"\n",
    "                      f\"range [{frame_tensor.min():.3f}, {frame_tensor.max():.3f}]\")\n",
    "                \n",
    "                if i >= 5:  # Process 5 frames for demo\n",
    "                    break\n",
    "        \n",
    "        print(\"✅ Tensor processing successful\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during tensor processing: {e}\")\n",
    "    \n",
    "    # Example 2: Batch tensor processing\n",
    "    print(\"\\n=== Batch Tensor Processing ===\")\n",
    "    try:\n",
    "        batcher = TensorFrameBatcher(batch_size=4, device=\"cpu\")\n",
    "        \n",
    "        with VideoFrameIterator(\n",
    "            video_path,\n",
    "            frame_skip=15,\n",
    "            output_format=\"tensor\",\n",
    "            normalize=True,\n",
    "            resize_frames=(224, 224)\n",
    "        ) as iterator:\n",
    "            \n",
    "            batch_count = 0\n",
    "            for frame_tensor, metadata in iterator:\n",
    "                batch_tensor, batch_metadata = batcher.add_frame(frame_tensor, metadata)\n",
    "                \n",
    "                if batch_tensor is not None:\n",
    "                    print(f\"Batch {batch_count}: Shape {batch_tensor.shape}, \"\n",
    "                          f\"Device {batch_tensor.device}\")\n",
    "                    batch_count += 1\n",
    "                    \n",
    "                    if batch_count >= 3:  # Process 3 batches for demo\n",
    "                        break\n",
    "            \n",
    "            # Get any remaining frames\n",
    "            remaining_batch, remaining_metadata = batcher.get_remaining()\n",
    "            if remaining_batch is not None:\n",
    "                print(f\"Final batch: Shape {remaining_batch.shape}\")\n",
    "        \n",
    "        print(\"✅ Batch tensor processing successful\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during batch tensor processing: {e}\")\n",
    "\n",
    "\n",
    "def demo_video_processing(video_path: str, output_dir: str = \"video_metadata_output\"):\n",
    "    \"\"\"\n",
    "    Demonstration of video frame processing with metadata collection\n",
    "    \n",
    "    Args:\n",
    "        video_path: Path to video file\n",
    "        output_dir: Directory to save metadata\n",
    "    \"\"\"\n",
    "    print(f\"Demo: Processing video {video_path}\")\n",
    "    \n",
    "    # Example 1: Basic frame iteration with metadata\n",
    "    print(\"\\n=== Basic Frame Iteration ===\")\n",
    "    metadata_list = []\n",
    "    \n",
    "    try:\n",
    "        with VideoFrameIterator(\n",
    "            video_path, \n",
    "            frame_skip=10,  # Process every 10th frame\n",
    "            collect_frame_stats=True,\n",
    "            resize_frames=(640, 480)\n",
    "        ) as iterator:\n",
    "            \n",
    "            for i, (frame, metadata) in enumerate(iterator):\n",
    "                metadata_list.append(metadata)\n",
    "                \n",
    "                print(f\"Frame {metadata.frame_number}: \"\n",
    "                      f\"{metadata.timestamp:.2f}s, \"\n",
    "                      f\"brightness={metadata.custom_metadata.get('mean_brightness', 0):.1f}\")\n",
    "                \n",
    "                # Stop after processing 20 frames for demo\n",
    "                if i >= 19:\n",
    "                    break\n",
    "        \n",
    "        print(f\"Processed {len(metadata_list)} frames\")\n",
    "        \n",
    "        # Save metadata\n",
    "        output_path = Path(output_dir)\n",
    "        output_path.mkdir(exist_ok=True)\n",
    "        \n",
    "        iterator_instance = VideoFrameIterator(video_path)\n",
    "        iterator_instance.save_metadata_batch(metadata_list, output_path / \"demo_metadata.json\")\n",
    "        iterator_instance.close()\n",
    "        \n",
    "        print(f\"Metadata saved to {output_path / 'demo_metadata.json'}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during processing: {e}\")\n",
    "    \n",
    "    # Example 2: Both formats (numpy + tensor)\n",
    "    print(\"\\n=== Both Formats Processing ===\")\n",
    "    try:\n",
    "        with VideoFrameIterator(\n",
    "            video_path,\n",
    "            frame_skip=20,\n",
    "            output_format=\"both\",\n",
    "            normalize=True,\n",
    "            resize_frames=(224, 224)\n",
    "        ) as iterator:\n",
    "            \n",
    "            for i, ((frame_np, frame_tensor), metadata) in enumerate(iterator):\n",
    "                print(f\"Frame {metadata.frame_number}: \"\n",
    "                      f\"NumPy {frame_np.shape}, Tensor {frame_tensor.shape}\")\n",
    "                \n",
    "                if i >= 3:\n",
    "                    break\n",
    "        \n",
    "        print(\"✅ Both formats processing successful\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during both formats processing: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description=\"Video Frame Iterator with Metadata Collection and Tensor Support\")\n",
    "    parser.add_argument('video_path', help='Path to video file')\n",
    "    parser.add_argument('--output_dir', default='video_metadata_output', \n",
    "                       help='Output directory for metadata')\n",
    "    parser.add_argument('--frame_skip', type=int, default=1, \n",
    "                       help='Process every Nth frame (default: 1)')\n",
    "    parser.add_argument('--start_frame', type=int, default=0, \n",
    "                       help='Starting frame number')\n",
    "    parser.add_argument('--end_frame', type=int, \n",
    "                       help='Ending frame number (optional)')\n",
    "    parser.add_argument('--resize', nargs=2, type=int, metavar=('WIDTH', 'HEIGHT'),\n",
    "                       help='Resize frames to specified dimensions')\n",
    "    parser.add_argument('--output_format', choices=['numpy', 'tensor', 'both'], \n",
    "                       default='numpy', help='Output format for frames')\n",
    "    parser.add_argument('--normalize', action='store_true', \n",
    "                       help='Normalize pixel values to [0, 1] range')\n",
    "    parser.add_argument('--device', default='cpu', \n",
    "                       help='Device for tensor operations (cpu, cuda, etc.)')\n",
    "    parser.add_argument('--demo', action='store_true', \n",
    "                       help='Run demonstration mode')\n",
    "    parser.add_argument('--demo_tensor', action='store_true',\n",
    "                       help='Run tensor demonstration mode')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    if args.demo:\n",
    "        demo_video_processing(args.video_path, args.output_dir)\n",
    "    elif args.demo_tensor:\n",
    "        demo_tensor_processing(args.video_path, args.output_dir)\n",
    "    else:\n",
    "        # Basic usage\n",
    "        resize_dims = tuple(args.resize) if args.resize else None\n",
    "        \n",
    "        print(f\"Processing video: {args.video_path}\")\n",
    "        print(f\"Output format: {args.output_format}\")\n",
    "        metadata_list = []\n",
    "        \n",
    "        try:\n",
    "            with VideoFrameIterator(\n",
    "                args.video_path,\n",
    "                frame_skip=args.frame_skip,\n",
    "                start_frame=args.start_frame,\n",
    "                end_frame=args.end_frame,\n",
    "                resize_frames=resize_dims,\n",
    "                output_format=args.output_format,\n",
    "                normalize=args.normalize,\n",
    "                device=args.device\n",
    "            ) as iterator:\n",
    "                \n",
    "                for frame_data, metadata in iterator:\n",
    "                    metadata_list.append(metadata)\n",
    "                    \n",
    "                    # Handle different output formats\n",
    "                    if args.output_format == \"numpy\":\n",
    "                        frame_info = f\"NumPy shape: {frame_data.shape}\"\n",
    "                    elif args.output_format == \"tensor\":\n",
    "                        frame_info = f\"Tensor shape: {frame_data.shape}, device: {frame_data.device}\"\n",
    "                    else:  # both\n",
    "                        frame_np, frame_tensor = frame_data\n",
    "                        frame_info = f\"NumPy: {frame_np.shape}, Tensor: {frame_tensor.shape}\"\n",
    "                    \n",
    "                    if len(metadata_list) % 100 == 0:\n",
    "                        progress = iterator.get_progress_info()\n",
    "                        print(f\"Progress: {progress['progress_percentage']:.1f}% \"\n",
    "                              f\"({progress['frames_processed']} frames) - {frame_info}\")\n",
    "            \n",
    "            # Save results\n",
    "            output_path = Path(args.output_dir)\n",
    "            output_path.mkdir(exist_ok=True)\n",
    "            \n",
    "            video_name = Path(args.video_path).stem\n",
    "            metadata_file = output_path / f\"{video_name}_metadata.json\"\n",
    "            \n",
    "            metadata_dicts = [metadata.to_dict() for metadata in metadata_list]\n",
    "            with open(metadata_file, 'w') as f:\n",
    "                json.dump(metadata_dicts, f, indent=2, default=str)\n",
    "            \n",
    "            print(f\"\\nCompleted! Processed {len(metadata_list)} frames\")\n",
    "            print(f\"Metadata saved to: {metadata_file}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa53cb0-8112-4a05-9f41-8d6f3571dfb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
